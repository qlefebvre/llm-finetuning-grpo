{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RLHF for LLMs: DPO and GRPO on Qwen2.5-0.5B\n",
        "\n",
        "This notebook demonstrates how to improve a small open LLM using **Reinforcement Learning from Human Feedback (RLHF)**.  \n",
        "We focus on two efficient preference-optimization techniques:\n",
        "- **DPO (Direct Preference Optimization)**\n",
        "- **GRPO (Generalized Reward Policy Optimization)**\n",
        "\n",
        "These methods align model responses with human-like preferences, building on top of the SFT model from the previous notebook.\n"
      ],
      "metadata": {
        "id": "iefwEX9vbgdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objectives\n",
        "\n",
        "- Load a small quantized LLM (Qwen2.5-0.5B)\n",
        "- Prepare preference data with \"chosen\" and \"rejected\" answers\n",
        "- Fine-tune with **Direct Preference Optimization (DPO)**\n",
        "- Optionally test **GRPO** for reward-based learning\n",
        "- Compare model generations before and after alignment\n"
      ],
      "metadata": {
        "id": "9C0EEnyHbjn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Uncomment the cell below if you are running this notebook on Colab or a fresh environment.\n"
      ],
      "metadata": {
        "id": "1POR7J2UbxIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone course repo (needed because we use its DPOTrainer)\n",
        "!git clone https://github.com/BounharAbdelaziz/RLHF.git"
      ],
      "metadata": {
        "id": "oh6MzRFQ4ezS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r RLHF/requirements.txt"
      ],
      "metadata": {
        "id": "hd_npukP4qBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate peft bitsandbytes datasets trl"
      ],
      "metadata": {
        "id": "_GjJ8X4xbuSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8AyeUIktUeV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "\n",
        "# optional tracking\n",
        "USE_WANDB = False\n",
        "if USE_WANDB:\n",
        "    import wandb\n",
        "    os.environ[\"WANDB_PROJECT\"] = \"RLHF\"\n",
        "    wandb.login()\n",
        "\n",
        "# dataset / model config\n",
        "DATASET_PATH = \"AIffl/french_orca_dpo_pairs\"\n",
        "LIMIT = 2_000\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "SEED = 1998\n",
        "\n",
        "MAX_PROMPT_LEN = 1024\n",
        "MAX_LENGTH = MAX_PROMPT_LEN + 512\n",
        "RUN_NAME = f\"DPO-french-orca-{MODEL_NAME.split('/')[-1]}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2czRsmsAQEF4"
      },
      "source": [
        "## (Optional) Experiment tracking\n",
        "\n",
        "You can log metrics to Weights & Biases (W&B) if you have an account.  \n",
        "This is optional ‚Äî the notebook runs without it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From SFT to RLHF\n",
        "\n",
        "Supervised Fine-Tuning (SFT) teaches a model to imitate examples, but it doesn't ensure the responses are *preferred*.  \n",
        "Reinforcement Learning from Human Feedback (RLHF) introduces a **preference dataset**, where each sample includes:\n",
        "- a **prompt**\n",
        "- a **chosen** (preferred) answer\n",
        "- a **rejected** (less preferred) answer\n",
        "\n",
        "The model learns to score the *chosen* higher than the *rejected*.  \n",
        "We use the **DPO** method to do this efficiently without a reward model or full RL.\n"
      ],
      "metadata": {
        "id": "8My7SATRcdr6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS_Fgnc4tUeW"
      },
      "source": [
        "## Load the base SFT model\n",
        "\n",
        "We use the 4-bit quantized **Qwen/Qwen2.5-0.5B-Instruct** model so that DPO training stays feasible on a single GPU.  \n",
        "The tokenizer will also be used to apply the chat template during data preparation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx0a87p1tUeW"
      },
      "outputs": [],
      "source": [
        "# Quantization configuration\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,)\n",
        "\n",
        "# Load the model to finetune\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "# to avoid warning\n",
        "model.config.use_cache = False\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "# Set padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wb1mjaltUeW"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "### üí¨ Chat templates & converting data to `messages`\n",
        "\n",
        "Modern instruction-tuned models (including **Qwen2.5-0.5B-Instruct**) expect inputs in a **chat format** and rely on a **tokenizer chat template** to turn structured messages into the exact token sequence the model was trained on. In practice, you should **not** hand-craft special tokens; instead, pass a list of `{role, content}` messages to the tokenizer and let `apply_chat_template(...)` do the right thing.\n",
        "\n",
        "#### Why a chat template?\n",
        "- Ensures your prompts match the **pretraining/finetuning format** (system/user/assistant turns, BOS/EOS, separators).\n",
        "- Minimizes prompt drift across libraries and models.\n",
        "- Makes it easy to add **system instructions** (e.g., ‚ÄúYou are a helpful assistant that answers in French.‚Äù).\n",
        "\n",
        "#### Message structure\n",
        "Each example becomes an ordered list of chat turns:\n",
        "```python\n",
        "messages = [\n",
        "  {\"role\": \"system\", \"content\": \"Tu es un assistant utile. R√©ponds en fran√ßais.\"},\n",
        "  {\"role\": \"user\", \"content\": \"Explique la diff√©rence entre LoRA et le fine-tuning complet.\"},\n",
        "  {\"role\": \"assistant\", \"content\": \"LoRA adapte un petit sous-espace de poids, alors que...\"}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW5vfRm0tUeW"
      },
      "outputs": [],
      "source": [
        "def preprocess_for_dpo(example):\n",
        "    # build chat-like prompt\n",
        "    messages = []\n",
        "    if example.get(\"system\") and example[\"system\"].strip():\n",
        "        messages.append({\"role\": \"system\", \"content\": example[\"system\"]})\n",
        "    messages.append({\"role\": \"user\", \"content\": example[\"question\"]})\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"chosen\": example[\"chosen\"],\n",
        "        \"rejected\": example[\"rejected\"],\n",
        "    }\n",
        "\n",
        "dataset = load_dataset(DATASET_PATH, split=\"train\").shuffle(seed=SEED).select(range(LIMIT))\n",
        "original_columns = dataset.column_names\n",
        "\n",
        "dpo_dataset = dataset.map(\n",
        "    preprocess_for_dpo,\n",
        "    remove_columns=original_columns,\n",
        ")\n",
        "\n",
        "def filter_length(example):\n",
        "    prompt_len = len(tokenizer(example[\"prompt\"]).input_ids)\n",
        "    chosen_len = len(tokenizer(example[\"chosen\"]).input_ids)\n",
        "    rejected_len = len(tokenizer(example[\"rejected\"]).input_ids)\n",
        "    return prompt_len + max(chosen_len, rejected_len) < MAX_LENGTH\n",
        "\n",
        "dpo_dataset = dpo_dataset.filter(filter_length)\n",
        "print(dpo_dataset[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75nrGIGktUeW"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "Training will mirror `01_instruction_finetuning_qwen.ipynb`, but we‚Äôll switch from SFT to **off-policy DPO** using the `trl` library. Concretely, we‚Äôll instantiate a **policy model** (trainable) and a **reference model** (frozen) and optimize with the DPO objective so the policy prefers **chosen** over **rejected** responses for the same prompt.\n",
        "\n",
        "### What we‚Äôll use\n",
        "- **TRL**: `DPOConfig`, `DPOTrainer`\n",
        "- **PEFT**: LoRA adapters on top of the base **Qwen2.5-0.5B-Instruct**\n",
        "- **Quantization**: 4-bit (QLoRA-style) to fit on small GPUs\n",
        "- **Logging**: W&B for metrics, configs, and artifacts\n",
        "\n",
        "### Expected dataset columns\n",
        "- `prompt` (or `messages`): the shared context (system+user turns)\n",
        "- `chosen`: assistant reply preferred by annotators\n",
        "- `rejected`: less-preferred reply\n",
        "> If you‚Äôre keeping everything in chat format, we‚Äôll pass lists of `{role, content}` and rely on `tokenizer.apply_chat_template(...)` inside the collator.\n",
        "\n",
        "### Minimal training flow\n",
        "1. Load tokenizer with the **chat template** and enable 4-bit loading of the base model.\n",
        "2. Wrap the model with **LoRA** (target attention/MLP modules).\n",
        "3. Build a `datasets.Dataset` that yields `(prompt/messages, chosen, rejected)`.\n",
        "4. Define `DPOConfig` (batch size, lr, epochs, `beta`, logging/saving/eval cadence).\n",
        "5. Create `DPOTrainer(policy_model, ref_model, tokenizer, train_dataset, eval_dataset, **config)`.\n",
        "6. Call `trainer.train()`; optional `trainer.evaluate()` and `trainer.save_model()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJeBW-l6tUeW"
      },
      "outputs": [],
      "source": [
        "# LoRA configuration - targeting the correct modules for Qwen2.5\n",
        "peft_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ], # Target all MLP layers\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ") # fill the gap\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Training configuration\n",
        "training_args = DPOConfig(\n",
        "    beta=0.1,  # DPO temperature parameter\n",
        "    learning_rate=5e-6,\n",
        "    max_prompt_length=MAX_PROMPT_LEN,\n",
        "    max_length=MAX_LENGTH,\n",
        "    per_device_train_batch_size=1,  # Reduced for memory\n",
        "    gradient_accumulation_steps=4,  # Increased to maintain effective batch size of 4 (1*4)\n",
        "    num_train_epochs=1,\n",
        "    max_grad_norm=1.0,\n",
        "    logging_steps=1,\n",
        "    save_steps=100,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_8bit\",  # More memory efficient\n",
        "    warmup_ratio=0.03, # 3% of the steps will be just a warmup\n",
        "    save_strategy=\"steps\",\n",
        "    output_dir=\"./dpo_model\",\n",
        "    report_to=\"none\",\n",
        "    run_name=RUN_NAME,\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_pin_memory=False,\n",
        "    fp16=True,  # Enable mixed precision\n",
        ")\n",
        "\n",
        "# Initialize the trainer - Note: no ref_model needed when using peft_config\n",
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    peft_config=peft_config,  # This automatically handles reference model\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=dpo_dataset,\n",
        ")\n",
        "\n",
        "# Print a sample to verify preprocessing\n",
        "print(\"Sample from dataset:\")\n",
        "print(f\"Prompt: {dpo_dataset[0]['prompt']}\")\n",
        "print(f\"Chosen: {dpo_dataset[0]['chosen']}\")\n",
        "print(f\"Rejected: {dpo_dataset[0]['rejected']}\")\n",
        "\n",
        "# Train\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm3zJDWVtUeW"
      },
      "outputs": [],
      "source": [
        "# merge LoRA adapters with the base model\n",
        "save_path = \"dpo_model/final_merged_dpo_model\"\n",
        "\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkbrN3GltUeW"
      },
      "source": [
        "## Model Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKtCZEoxtUeW"
      },
      "outputs": [],
      "source": [
        "def generate(model, tokenizer, prompt, max_new_tokens=200):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "test_prompt = \"<human>: Donne-moi 3 conseils pour s√©curiser une API.\\n<assistant>:\"\n",
        "print(generate(model, tokenizer, test_prompt))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb2KC8TJtUeX"
      },
      "source": [
        "# Part II ‚Äì GRPO\n",
        "\n",
        "In this section we show how the same model can be optimized with a reinforcement-style method, **GRPO**.  \n",
        "This is optional ‚Äî DPO is often enough for small alignment tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reuse the model and tokenizer loaded in the DPO section\n",
        "grpo_model = model\n",
        "grpo_tokenizer = tokenizer\n",
        "\n",
        "# We will build a very small example dataset for GRPO right after\n"
      ],
      "metadata": {
        "id": "kVVeXm0zu9je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U95rr5j-tUeX"
      },
      "outputs": [],
      "source": [
        "# Load a tiny math dataset for the GRPO demo\n",
        "from datasets import load_dataset\n",
        "\n",
        "grpo_ds = load_dataset(\"openai/gsm8k\", 'main', split=\"train[:200]\")\n",
        "\n",
        "def build_prompt(example):\n",
        "    return f\"<human>: Solve the following math problem step by step.\\n{example['question']}\\n<assistant>:\"\n",
        "\n",
        "grpo_ds = grpo_ds.map(lambda ex: {\"prompt\": build_prompt(ex)})\n",
        "print(grpo_ds[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvwShnXPhJxI"
      },
      "outputs": [],
      "source": [
        "grpo_ds[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AsDSYi3tUeX"
      },
      "source": [
        "## Reward Function Design\n",
        "\n",
        "We‚Äôll use **two simple rewards** during GRPO rollouts:\n",
        "\n",
        "1. **Format reward** ‚Äî checks that the **last non-empty line** is exactly in the form  \n",
        "   `<answer>NUMBER</answer>`  \n",
        "   - Score: **1** if correct format, **0** otherwise.\n",
        "\n",
        "2. **Correctness reward** ‚Äî checks whether the extracted number matches the gold answer.  \n",
        "   - Score: **2** if correct, **0** otherwise.\n",
        "\n",
        "Total reward per sample ‚àà {0, 1, 2, 3}.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjvdHWcVtUeX"
      },
      "outputs": [],
      "source": [
        "def extract_xml_answer(text: str) -> str:\n",
        "    match = re.search(r\"<answer>(\\d+)</answer>\", text)\n",
        "    if match:\n",
        "        answer = match.group(1)\n",
        "    else:\n",
        "        answer = \"\" # Return empty string if not found\n",
        "    return answer\n",
        "\n",
        "def format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has the correct format.\"\"\"\n",
        "    pattern = r\"^(?:[^\\r\\n]*\\r?\\n)+<answer>\\d+</answer>\\r?\\n?$\"\n",
        "    responses = completions\n",
        "    matches = [bool(re.match(pattern, r)) for r in responses]\n",
        "    return [1.0 if match else 0.0 for match in matches]\n",
        "\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the answer is correct.\"\"\"\n",
        "    responses = completions\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoWWcLFhtUeX"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAcuDydttUeY"
      },
      "outputs": [],
      "source": [
        "# Optional GRPO skeleton (commented to keep notebook runnable)\n",
        "# from trl import GRPOTrainer, GRPOConfig\n",
        "#\n",
        "# grpo_config = GRPOConfig(\n",
        "#     max_steps=200,\n",
        "# )\n",
        "#\n",
        "# grpo_args = TrainingArguments(\n",
        "#     output_dir=\"./grpo_model\",\n",
        "#     per_device_train_batch_size=1,\n",
        "#     gradient_accumulation_steps=4,\n",
        "#     num_train_epochs=1,\n",
        "#     logging_steps=10,\n",
        "#     report_to=\"none\",\n",
        "# )\n",
        "#\n",
        "# grpo_trainer = GRPOTrainer(\n",
        "#     model=grpo_model,\n",
        "#     args=grpo_args,\n",
        "#     tokenizer=grpo_tokenizer,\n",
        "#     train_dataset=grpo_ds,\n",
        "#     grpo_config=grpo_config,\n",
        "# )\n",
        "# grpo_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "Quick qualitative check on a few prompts (same model as above).\n"
      ],
      "metadata": {
        "id": "qIpnugPkeStI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEE-nS0I33TW"
      },
      "outputs": [],
      "source": [
        "def generate(model, tokenizer, prompt, max_new_tokens=200):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "test_prompts = [\n",
        "    \"<human>: Solve 12 + 35. Show the result only.\\n<assistant>:\",\n",
        "    \"<human>: Explique le principe du RLHF en 5 phrases.\\n<assistant>:\",\n",
        "]\n",
        "\n",
        "for p in test_prompts:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"PROMPT:\", p)\n",
        "    print(generate(model, tokenizer, p))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: interactive demo (commented out to keep notebook runnable)\n",
        "# !pip install gradio -q\n",
        "# import gradio as gr\n",
        "#\n",
        "# def dpo_chat(prompt):\n",
        "#     text = f\"<human>: {prompt}\\n<assistant>:\"\n",
        "#     inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "#\n",
        "# demo = gr.Interface(\n",
        "#     fn=dpo_chat,\n",
        "#     inputs=gr.Textbox(label=\"Your prompt\"),\n",
        "#     outputs=gr.Textbox(label=\"Model answer\"),\n",
        "#     title=\"Qwen2.5-0.5B ‚Äì aligned demo\",\n",
        "# )\n",
        "# demo.launch()"
      ],
      "metadata": {
        "id": "TJ9u2p_9tsKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "- we loaded a 4-bit Qwen2.5-0.5B-Instruct model\n",
        "- we prepared a French DPO dataset (prompt, chosen, rejected)\n",
        "- we ran DPO with LoRA adapters and no external logger\n",
        "- we optionally showed how to structure a GRPO experiment\n",
        "\n",
        "This notebook is meant to be lightweight and easy to run on Colab / single GPU."
      ],
      "metadata": {
        "id": "pnDQANCUi6hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reproducibility vs. demo quality\n",
        "\n",
        "The goal of this notebook is to provide a **fully runnable** DPO/GRPO example on top of a quantized Qwen2.5-0.5B model.\n",
        "The original lab also included a Gradio-based chat UI with slightly different prompting and enough GPU memory, which\n",
        "produced cleaner generations.\n",
        "\n",
        "In lightweight environments (Colab, 4-bit, small context) you may observe:\n",
        "- verbose or generic answers,\n",
        "- occasional language mixing,\n",
        "- sensitivity to the prompt template.\n",
        "\n",
        "For reference, the README includes screenshots of the Gradio demo in the ‚Äúgood‚Äù environment.\n"
      ],
      "metadata": {
        "id": "7uDh7pmWDPpw"
      }
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}